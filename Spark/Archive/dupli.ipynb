{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d079a63-2f45-4949-998b-dc513d9637da",
   "metadata": {},
   "source": [
    "# Configured Spark, PostgreSQL and MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd062a6-722f-4369-9376-054a62a7cb2c",
   "metadata": {},
   "source": [
    "##### 1. PySpark Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7457f6-5f6c-4bfe-866e-6a3bfc43d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783f70a-3b40-4ef5-814c-e3a0873b8974",
   "metadata": {},
   "source": [
    "##### 2. Configuring SparkSession for PostgreSQl and MongoDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e55ba6-18db-4ea7-8e72-581326509633",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"restaurant\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.jars\", \"/Users/deependrashekhawat/jars/postgresql-42.2.21.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426e948-1ee1-4d8a-9bcc-6aafa01e9fd5",
   "metadata": {},
   "source": [
    "# Processing Restaurant Data from PostgreSQL DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018099b-02db-46e9-9488-be4c13751d9b",
   "metadata": {},
   "source": [
    "##### 1. Creating cursor and executing SQL query to fetch all the restaurant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba21ca4-a62f-465f-9470-8af80071b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host=\"localhost\", database=\"testrestaurant\", user=\"postgres\", password=\"Welcome@1\", port=5436)\n",
    "curr = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b05110-bec9-41cc-833f-e98e72e5cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr.execute(\"\"\"\n",
    "SELECT rs.restaurant_id, restaurant_name, street, city, state, postal_code, latitude, longitude, stars, review_count, cuisine_name\n",
    "FROM restaurantcuisine rs\n",
    "JOIN restaurants r ON (rs.restaurant_id = r.restaurant_id)\n",
    "JOIN address a ON (r.address_id = a.address_id)\n",
    "JOIN cuisines c ON (rs.cuisine_id = c.cuisine_id);\n",
    "\"\"\")\n",
    "restaurantCuisineRaw = curr.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411901e-0c8a-4a51-b3d7-013036f24e5e",
   "metadata": {},
   "source": [
    "##### 2. Creating a Raw DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2dd044-9e8e-40a7-bc92-f5a6e9db113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"restaurant_id\", \"restaurant_name\", \"street\", \"city\", \"state\", \\\n",
    "           \"postal_code\", \"latitude\", \"longitude\", \"stars\", \"review_count\", \"cuisine_name\"]\n",
    "dfRestCusRaw = my_spark.createDataFrame(data=restaurantCuisineRaw, schema = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a9d77-2072-4d78-9a5e-f420e9bff002",
   "metadata": {},
   "source": [
    "##### 3. Processing Restaurant data to fetch top 10 for each cuisine based on City and State."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d989397-c4ab-4a75-9b2a-52f59ac685f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"city\", \"state\", \"cuisine_name\")\\\n",
    "                    .orderBy(F.col(\"stars\").desc(), F.col(\"review_count\").desc())\n",
    "\n",
    "max_number_of_rows_per_partition = 10\n",
    "\n",
    "dfRestCusProcessed = dfRestCusRaw.withColumn(\"row_number\", F.row_number().over(window_spec))\\\n",
    "  .filter(F.col(\"row_number\") <= max_number_of_rows_per_partition)\\\n",
    "  .drop(\"row_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15954d48-d3af-4ad9-afde-79acd42f604d",
   "metadata": {},
   "source": [
    "# User based Restaurant Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c9fe3-88e7-4b7f-a677-5c72fdb34c29",
   "metadata": {},
   "source": [
    "##### 1. Reading User data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c5ac5e-6be4-48af-87ca-9bca1e8260de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUser = my_spark.read \\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/hungryApp.user\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dabb57-9b92-4f63-a9df-9b39b2a968e5",
   "metadata": {},
   "source": [
    "##### 2. Function to generate restaurant JSON when called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26859518-32e4-42d1-9e37-777231b06172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topRestaurant(cuisines, city, state):\n",
    "    cuisineRestaurant = {}\n",
    "    for cuisine in cuisines:\n",
    "        cuisineRestaurant[cuisine] = dfRestCusProcessed \\\n",
    "                                        .filter(F.col('cuisine_name').isin(cuisine.capitalize())) \\\n",
    "                                        .filter(F.col(\"city\") == city.capitalize() ) \\\n",
    "                                        .filter(F.col(\"state\") == state.upper() ) \\\n",
    "                                        .toJSON().take(10)\n",
    "    return cuisineRestaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e66241-6c6a-4617-85ff-3369e63ef9b8",
   "metadata": {},
   "source": [
    "##### 3. Making a function call to get JSON and creating a tupple of each user and there recommended restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18d074c-bf53-486c-b6dc-48dd148c2107",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = dfUser.rdd.map(lambda x: (x._id, x.city, x.state, x.preference)).collect() #Generating a List of Users to iterate over\n",
    "\n",
    "userRecommendation = []\n",
    "\n",
    "for i, row in enumerate(users):\n",
    "    userId = row[0]\n",
    "    userCity = row[1].capitalize()\n",
    "    userState = row[2].upper()\n",
    "    userCuisines = [row.capitalize() for row in row[3]]\n",
    "    \n",
    "    userRecommendation.append((userId, topRestaurant(userCuisines, userCity, userState)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c9022-ada3-4437-aa97-a07b98f883fe",
   "metadata": {},
   "source": [
    "##### 4. Generating a user recommendation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "114a7e6d-5d22-4862-b5b6-99c41d39c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUserRecommendation = my_spark.createDataFrame(data=userRecommendation, schema = [\"_id\", \"cuisines\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5def0355-5145-4c74-93ca-c4bb48501139",
   "metadata": {},
   "source": [
    "##### 5. Writing top 10 restaurant recommendation for a user to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d4003ec-fa65-4a60-a008-d72b22795d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUserRecommendation.write \\\n",
    ".format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".option(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/hungryApp.restaurantRecommendation\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a206e-5c51-4762-bd73-bb663dc8becb",
   "metadata": {},
   "source": [
    "# City, State and Cuisine based Top 10 Restaurant Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6e369d49-a55e-479f-a7d0-2b9ad2c2c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCityCusRest = dfRestCusProcessed.groupBy(\"city\", \"state\", \"cuisine_name\") \\\n",
    "                    .agg(F.collect_list(F.struct(\"restaurant_id\", \"restaurant_name\", \"street\", \"city\", \"state\", \\\n",
    "                           \"postal_code\", \"latitude\", \"longitude\", \"stars\", \"review_count\", \"cuisine_name\")).alias(\"restaurants\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5e371fb6-2227-4616-b4cc-898be14ec9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCityCusRest.write \\\n",
    ".format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".option(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/hungryApp.restaurantCity\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729e80e-cfe2-42e7-8faa-af6c424bd035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
